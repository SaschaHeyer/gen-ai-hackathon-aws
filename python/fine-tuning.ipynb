{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Amazon Titan model in Bedrock for summarization task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.226.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: datasets in /Users/sascha/miniconda3/lib/python3.11/site-packages (2.15.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from sagemaker) (23.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.34.142 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from sagemaker) (1.34.147)\n",
      "Collecting cloudpickle==2.2.1 (from sagemaker)\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting google-pasta (from sagemaker)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from sagemaker) (1.26.4)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from sagemaker) (4.25.3)\n",
      "Collecting smdebug-rulesconfig==1.0.1 (from sagemaker)\n",
      "  Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting importlib-metadata<7.0,>=1.4.0 (from sagemaker)\n",
      "  Downloading importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from sagemaker) (24.1)\n",
      "Requirement already satisfied: pandas in /Users/sascha/miniconda3/lib/python3.11/site-packages (from sagemaker) (2.1.3)\n",
      "Collecting pathos (from sagemaker)\n",
      "  Downloading pathos-0.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting schema (from sagemaker)\n",
      "  Downloading schema-0.7.7-py2.py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from sagemaker) (6.0.1)\n",
      "Requirement already satisfied: jsonschema in /Users/sascha/miniconda3/lib/python3.11/site-packages (from sagemaker) (4.23.0)\n",
      "Requirement already satisfied: platformdirs in /Users/sascha/miniconda3/lib/python3.11/site-packages (from sagemaker) (4.2.2)\n",
      "Collecting tblib<4,>=1.7.0 (from sagemaker)\n",
      "  Downloading tblib-3.0.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from sagemaker) (2.1.0)\n",
      "Requirement already satisfied: requests in /Users/sascha/miniconda3/lib/python3.11/site-packages (from sagemaker) (2.32.3)\n",
      "Collecting docker (from sagemaker)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm in /Users/sascha/miniconda3/lib/python3.11/site-packages (from sagemaker) (4.65.0)\n",
      "Requirement already satisfied: psutil in /Users/sascha/miniconda3/lib/python3.11/site-packages (from sagemaker) (5.9.0)\n",
      "Requirement already satisfied: filelock in /Users/sascha/miniconda3/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-17.0.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/sascha/miniconda3/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
      "Collecting tqdm (from sagemaker)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: xxhash in /Users/sascha/miniconda3/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/sascha/miniconda3/lib/python3.11/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /Users/sascha/miniconda3/lib/python3.11/site-packages (from datasets) (3.9.5)\n",
      "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
      "  Downloading huggingface_hub-0.24.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.147 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from boto3<2.0,>=1.34.142->sagemaker) (1.34.147)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from boto3<2.0,>=1.34.142->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from boto3<2.0,>=1.34.142->sagemaker) (0.10.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from requests->sagemaker) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from requests->sagemaker) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from requests->sagemaker) (2024.7.4)\n",
      "Requirement already satisfied: six in /Users/sascha/miniconda3/lib/python3.11/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from jsonschema->sagemaker) (2023.11.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from jsonschema->sagemaker) (0.31.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from jsonschema->sagemaker) (0.13.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from pandas->sagemaker) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from pandas->sagemaker) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/sascha/miniconda3/lib/python3.11/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Collecting ppft>=1.7.6.8 (from pathos->sagemaker)\n",
      "  Downloading ppft-1.7.6.8-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pox>=0.3.4 (from pathos->sagemaker)\n",
      "  Downloading pox-0.3.4-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Downloading sagemaker-2.226.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
      "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.24.1-py3-none-any.whl (417 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.2/417.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
      "Downloading pyarrow-17.0.0-cp311-cp311-macosx_11_0_arm64.whl (27.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.2/27.2 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tblib-3.0.0-py3-none-any.whl (12 kB)\n",
      "Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pathos-0.3.2-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
      "Downloading pox-0.3.4-py3-none-any.whl (29 kB)\n",
      "Downloading ppft-1.7.6.8-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: schema, tqdm, tblib, smdebug-rulesconfig, pyarrow, ppft, pox, importlib-metadata, google-pasta, dill, cloudpickle, multiprocess, huggingface-hub, docker, pathos, datasets, sagemaker\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.65.0\n",
      "    Uninstalling tqdm-4.65.0:\n",
      "      Successfully uninstalled tqdm-4.65.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.1\n",
      "    Uninstalling pyarrow-14.0.1:\n",
      "      Successfully uninstalled pyarrow-14.0.1\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib_metadata 8.0.0\n",
      "    Uninstalling importlib_metadata-8.0.0:\n",
      "      Successfully uninstalled importlib_metadata-8.0.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.15\n",
      "    Uninstalling multiprocess-0.70.15:\n",
      "      Successfully uninstalled multiprocess-0.70.15\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.19.4\n",
      "    Uninstalling huggingface-hub-0.19.4:\n",
      "      Successfully uninstalled huggingface-hub-0.19.4\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.15.0\n",
      "    Uninstalling datasets-2.15.0:\n",
      "      Successfully uninstalled datasets-2.15.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.28.2 requires packaging<24,>=16.8, but you have packaging 24.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cloudpickle-2.2.1 datasets-2.20.0 dill-0.3.8 docker-7.1.0 google-pasta-0.2.0 huggingface-hub-0.24.1 importlib-metadata-6.11.0 multiprocess-0.70.16 pathos-0.3.2 pox-0.3.4 ppft-1.7.6.8 pyarrow-17.0.0 sagemaker-2.226.1 schema-0.7.7 smdebug-rulesconfig-1.0.1 tblib-3.0.0 tqdm-4.66.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade sagemaker datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data in Bedrock required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sascha/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 8.20k/8.20k [00:00<00:00, 19.8MB/s]\n",
      "Downloading data: 100%|██████████| 13.1M/13.1M [00:01<00:00, 12.3MB/s]\n",
      "Generating train split: 100%|██████████| 15011/15011 [00:00<00:00, 587100.87 examples/s]\n",
      "Filter: 100%|██████████| 15011/15011 [00:00<00:00, 250404.47 examples/s]\n",
      "Map: 100%|██████████| 1188/1188 [00:00<00:00, 25550.63 examples/s]\n",
      "Filter: 100%|██████████| 1188/1188 [00:00<00:00, 121408.15 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 77.24ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Please describe  what is oil and give me a list of it’s applications. An oil is any nonpolar chemical substance that is composed primarily of hydrocarbons and is hydrophobic (does not mix with water) & lipophilic (mixes with other oils). Oils are usually flammable and surface active. Most oils are unsaturated lipids that are liquid at room temperature.\\n\\nThe general definition of oil includes classes of chemical compounds that may be otherwise unrelated in structure, properties, and uses. Oils may be animal, vegetable, or petrochemical in origin, and may be volatile or non-volatile. They are used for food (e.g., olive oil), fuel (e.g., heating oil), medical purposes (e.g., mineral oil), lubrication (e.g. motor oil), and the manufacture of many types of paints, plastics, and other materials. Specially prepared oils are used in some religious ceremonies and rituals as purifying agents.', 'completion': 'An oil is a chemical substance that is composed primarily of hydrocarbons and may be animal, vegetable or petrochemical in origin.\\nOil is used in a wide range of applications and is essential to everyday human life. These are:\\nCooking - edible vegetable and animal oils are used for various purposes in cooking and food preparation\\nCosmetics - most facial cleansers, lotions and hair care products contain molecules that come from mineral and vegetable oils\\nFuel - crude oil is refined and converted to diesel, gasoline or jet fuel to power cars, trucks and planes\\nHeating - petrochemical oil is used for heating\\nPainting - oil is used as a supporting medium for paints\\nLubrication - oils are used in various engineering purposes as they do not easily adhere to other substance which makes them useful as lubricants\\nReligion - oil has been used throughout history as a religious medium. It is often considered a spiritually purifying agent and is used to anointing purposes\\nHealth - oils holds lots of fats and medical properties, for example fish oil holds the omega-3 fatty acid which helps with inflammation and reduces fat in the bloodstream'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Here we are using the Dolly dataset for fine-tuning the titan model for summarization task\n",
    "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "# Filter the dataset to include only summarization examples\n",
    "summarization_dataset = dolly_dataset.filter(lambda example: example[\"category\"] == \"summarization\")\n",
    "summarization_dataset = summarization_dataset.remove_columns(\"category\")\n",
    "\n",
    "# Create a new DataFrame with Bedrock supported format\n",
    "modified_df = summarization_dataset.map(\n",
    "    lambda example: {\"prompt\": f\"{example['instruction']} {example['context']}\",\n",
    "                     \"completion\": example[\"response\"]}\n",
    ")\n",
    "\n",
    "# Set max length to support Bedrock Customization Max token Length Quota\n",
    "max_length_per_row = 18000\n",
    "\n",
    "# Set max length to support Bedrock Customization Max token Length Quota\n",
    "max_row_count = 9000\n",
    "\n",
    "# Define a function to check if the total length of prompt and completion is within the specified max_length\n",
    "def within_length(example):\n",
    "    return len(example['prompt'] + example['completion']) <= max_length_per_row\n",
    "\n",
    "# Filter the DataFrame to include only examples within the specified max length\n",
    "modified_df = modified_df.filter(within_length)\n",
    "\n",
    "num_rows = len(modified_df)\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "\n",
    "\n",
    "# Take the maximum supported dataset size\n",
    "if num_rows > max_row_count:\n",
    "    modified_df = modified_df.select(range(max_row_count)) \n",
    "\n",
    "# Remove unnecessary columns\n",
    "modified_df = modified_df.remove_columns([\"instruction\", \"context\", \"response\"])\n",
    "\n",
    "# Dump the modified DataFrame to a JSON file \"train.jsonl\" in local directory\n",
    "modified_df.to_json(\"train.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "# Print the 11th example in the modified DataFrame (since Python uses 0-based indexing)\n",
    "print(modified_df[10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload training data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded train.jsonl to s3://bedrock-doit-demo/fine-tuning/train.jsonl\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from sagemaker import Session\n",
    "\n",
    "# Update this to your bucket name and make sure it exists in S3\n",
    "default_bucket = 'bedrock-doit-demo'\n",
    "\n",
    "# Create a session using the provided AWS SDK sessions and default bucket\n",
    "session = Session(boto_session=boto3.session.Session(),\n",
    "                  sagemaker_client=boto3.client('sagemaker'),\n",
    "                  sagemaker_runtime_client=boto3.client('runtime.sagemaker'),\n",
    "                  default_bucket=default_bucket)\n",
    "\n",
    "# Create an S3 resource using the AWS SDK\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Specify the path to the training data on your local machine\n",
    "train_data_path = 'train.jsonl'\n",
    "\n",
    "# Upload the training data to the specified S3 key prefix 'PreProcessed'\n",
    "s3_train_data = session.upload_data(path=train_data_path, key_prefix='fine-tuning')\n",
    "\n",
    "# Print a message indicating the successful upload\n",
    "print(f\"Uploaded {train_data_path} to {s3_train_data}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the bedrock training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParamValidationError",
     "evalue": "Parameter validation failed:\nMissing required parameter in input: \"roleArn\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParamValidationError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/Users/sascha/Desktop/development/amazon-bedrock-samples/hackathon-workshop/fine-tuning.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sascha/Desktop/development/amazon-bedrock-samples/hackathon-workshop/fine-tuning.ipynb#X26sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m outputDataConfig \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39ms3Uri\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39ms3://\u001b[39m\u001b[39m{\u001b[39;00mdefault_bucket\u001b[39m}\u001b[39;00m\u001b[39m/CustomModel/\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sascha/Desktop/development/amazon-bedrock-samples/hackathon-workshop/fine-tuning.ipynb#X26sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# Create a job for model customization\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sascha/Desktop/development/amazon-bedrock-samples/hackathon-workshop/fine-tuning.ipynb#X26sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m jobIdentifier \u001b[39m=\u001b[39m bedrock\u001b[39m.\u001b[39;49mcreate_model_customization_job(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sascha/Desktop/development/amazon-bedrock-samples/hackathon-workshop/fine-tuning.ipynb#X26sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     jobName\u001b[39m=\u001b[39;49mjobName,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sascha/Desktop/development/amazon-bedrock-samples/hackathon-workshop/fine-tuning.ipynb#X26sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     customModelName\u001b[39m=\u001b[39;49mcustomModelName,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sascha/Desktop/development/amazon-bedrock-samples/hackathon-workshop/fine-tuning.ipynb#X26sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39m#roleArn=roleArn,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sascha/Desktop/development/amazon-bedrock-samples/hackathon-workshop/fine-tuning.ipynb#X26sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     baseModelIdentifier\u001b[39m=\u001b[39;49mbaseModelIdentifierForProvisonedThroughput,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sascha/Desktop/development/amazon-bedrock-samples/hackathon-workshop/fine-tuning.ipynb#X26sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     hyperParameters\u001b[39m=\u001b[39;49mhyperParameters,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sascha/Desktop/development/amazon-bedrock-samples/hackathon-workshop/fine-tuning.ipynb#X26sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     trainingDataConfig\u001b[39m=\u001b[39;49mtrainingDataConfig,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sascha/Desktop/development/amazon-bedrock-samples/hackathon-workshop/fine-tuning.ipynb#X26sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     outputDataConfig\u001b[39m=\u001b[39;49moutputDataConfig\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sascha/Desktop/development/amazon-bedrock-samples/hackathon-workshop/fine-tuning.ipynb#X26sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sascha/Desktop/development/amazon-bedrock-samples/hackathon-workshop/fine-tuning.ipynb#X26sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# Print the identifier for the created job\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sascha/Desktop/development/amazon-bedrock-samples/hackathon-workshop/fine-tuning.ipynb#X26sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel customization job created with identifier: \u001b[39m\u001b[39m{\u001b[39;00mjobIdentifier\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/botocore/client.py:565\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/botocore/client.py:974\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[39mif\u001b[39;00m properties:\n\u001b[1;32m    971\u001b[0m     \u001b[39m# Pass arbitrary endpoint info with the Request\u001b[39;00m\n\u001b[1;32m    972\u001b[0m     \u001b[39m# for use during construction.\u001b[39;00m\n\u001b[1;32m    973\u001b[0m     request_context[\u001b[39m'\u001b[39m\u001b[39mendpoint_properties\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m properties\n\u001b[0;32m--> 974\u001b[0m request_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_to_request_dict(\n\u001b[1;32m    975\u001b[0m     api_params\u001b[39m=\u001b[39;49mapi_params,\n\u001b[1;32m    976\u001b[0m     operation_model\u001b[39m=\u001b[39;49moperation_model,\n\u001b[1;32m    977\u001b[0m     endpoint_url\u001b[39m=\u001b[39;49mendpoint_url,\n\u001b[1;32m    978\u001b[0m     context\u001b[39m=\u001b[39;49mrequest_context,\n\u001b[1;32m    979\u001b[0m     headers\u001b[39m=\u001b[39;49madditional_headers,\n\u001b[1;32m    980\u001b[0m )\n\u001b[1;32m    981\u001b[0m resolve_checksum_context(request_dict, operation_model, api_params)\n\u001b[1;32m    983\u001b[0m service_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_service_model\u001b[39m.\u001b[39mservice_id\u001b[39m.\u001b[39mhyphenize()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/botocore/client.py:1041\u001b[0m, in \u001b[0;36mBaseClient._convert_to_request_dict\u001b[0;34m(self, api_params, operation_model, endpoint_url, context, headers, set_user_agent_header)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_convert_to_request_dict\u001b[39m(\n\u001b[1;32m   1033\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1034\u001b[0m     api_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1039\u001b[0m     set_user_agent_header\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   1040\u001b[0m ):\n\u001b[0;32m-> 1041\u001b[0m     request_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_serializer\u001b[39m.\u001b[39;49mserialize_to_request(\n\u001b[1;32m   1042\u001b[0m         api_params, operation_model\n\u001b[1;32m   1043\u001b[0m     )\n\u001b[1;32m   1044\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client_config\u001b[39m.\u001b[39minject_host_prefix:\n\u001b[1;32m   1045\u001b[0m         request_dict\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mhost_prefix\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/botocore/validate.py:381\u001b[0m, in \u001b[0;36mParamValidationDecorator.serialize_to_request\u001b[0;34m(self, parameters, operation_model)\u001b[0m\n\u001b[1;32m    377\u001b[0m     report \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param_validator\u001b[39m.\u001b[39mvalidate(\n\u001b[1;32m    378\u001b[0m         parameters, operation_model\u001b[39m.\u001b[39minput_shape\n\u001b[1;32m    379\u001b[0m     )\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m report\u001b[39m.\u001b[39mhas_errors():\n\u001b[0;32m--> 381\u001b[0m         \u001b[39mraise\u001b[39;00m ParamValidationError(report\u001b[39m=\u001b[39mreport\u001b[39m.\u001b[39mgenerate_report())\n\u001b[1;32m    382\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_serializer\u001b[39m.\u001b[39mserialize_to_request(\n\u001b[1;32m    383\u001b[0m     parameters, operation_model\n\u001b[1;32m    384\u001b[0m )\n",
      "\u001b[0;31mParamValidationError\u001b[0m: Parameter validation failed:\nMissing required parameter in input: \"roleArn\""
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import uuid  # Import the 'uuid' module for generating a unique identifier\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Initialize Bedrock client\n",
    "bedrock = boto3.client(service_name='bedrock')\n",
    "\n",
    "# Get the SageMaker execution role\n",
    "#role = get_execution_role()\n",
    "\n",
    "# Set parameters\n",
    "customizationType = \"FINE_TUNING\"\n",
    "\n",
    "# Base model to use\n",
    "basemodelId = 'amazon.titan-text-express-v1'\n",
    "\n",
    "# Model ID for provisioned throughput\n",
    "# https://docs.aws.amazon.com/bedrock/latest/userguide/prov-thru-api.html\n",
    "baseModelIdentifierForProvisonedThroughput = \"arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-express-v1:0:8k\"\n",
    "job_prefix = \"customTitan\"\n",
    "\n",
    "# Update this to a valid roleARn\n",
    "roleArn = 'arn:aws:iam::303673948954:role/bedrock_dev'\n",
    "\n",
    "# Generate a unique identifier for the job and custom model name\n",
    "job_uuid = str(uuid.uuid4())[:8]  # Extracting the first 8 characters for brevity\n",
    "jobName = f\"{job_prefix}-{job_uuid}\"\n",
    "customModelName = f\"{job_prefix}-{job_uuid}\"\n",
    "\n",
    "hyperParameters = {\n",
    "    \"epochCount\": \"2\",\n",
    "    \"batchSize\": \"1\",\n",
    "    \"learningRate\": \"0.00001\",\n",
    "}\n",
    "\n",
    "# Specify the training data configuration using the previously uploaded S3 data\n",
    "trainingDataConfig = {\"s3Uri\": s3_train_data}\n",
    "\n",
    "# Specify the output data configuration for the custom model\n",
    "outputDataConfig = {\"s3Uri\": f\"s3://{default_bucket}/CustomModel/\"}\n",
    "\n",
    "# Create a job for model customization\n",
    "jobIdentifier = bedrock.create_model_customization_job(\n",
    "    jobName=jobName,\n",
    "    customModelName=customModelName,\n",
    "    roleArn=roleArn,\n",
    "    baseModelIdentifier=baseModelIdentifierForProvisonedThroughput,\n",
    "    hyperParameters=hyperParameters,\n",
    "    trainingDataConfig=trainingDataConfig,\n",
    "    outputDataConfig=outputDataConfig\n",
    ")\n",
    "\n",
    "# Print the identifier for the created job\n",
    "print(f\"Model customization job created with identifier: {jobIdentifier}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor the job till the status is shown as \"Completed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bedrock' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/sascha/Desktop/development/amazon-bedrock-samples/hackathon-workshop/fine-tuning.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sascha/Desktop/development/amazon-bedrock-samples/hackathon-workshop/fine-tuning.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m fine_tune_job \u001b[39m=\u001b[39m bedrock\u001b[39m.\u001b[39mget_model_customization_job(jobIdentifier\u001b[39m=\u001b[39mjobIdentifier[\u001b[39m'\u001b[39m\u001b[39mjobArn\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sascha/Desktop/development/amazon-bedrock-samples/hackathon-workshop/fine-tuning.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(fine_tune_job[\u001b[39m'\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bedrock' is not defined"
     ]
    }
   ],
   "source": [
    "fine_tune_job = bedrock.get_model_customization_job(jobIdentifier=jobIdentifier['jobArn'])\n",
    "print(fine_tune_job['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create provisioned no-commit throughput for the custom model (Only run the following once the status of the above job is shown as \"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customModelId=fine_tune_job['outputModelArn']\n",
    "\n",
    "\n",
    "provisionedModelName = f\"{job_prefix}-provisioned-{job_uuid}\"\n",
    "\n",
    "# Create the provisioned capacity without passing any commitment option\n",
    "provisionedModelArn = bedrock.create_provisioned_model_throughput(\n",
    "    modelUnits=1,\n",
    "    provisionedModelName=provisionedModelName, \n",
    "    modelId=customModelId\n",
    "   )['provisionedModelArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the provisoned capacity creation status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Provisioned model status untill it's completed\n",
    "provisionedModelStatus = bedrock.get_provisioned_model_throughput(provisionedModelId=provisionedModelArn)\n",
    "print (provisionedModelStatus['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference on the custom provisioned model and the base model via bedrock and observe the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Initialize Bedrock Runtime client in the specified region\n",
    "bedrockRuntime = boto3.client(service_name='bedrock-runtime', region_name='us-east-1')\n",
    "\n",
    "# Sample request body containing text for summarization and parameters for model inference\n",
    "body = json.dumps({\n",
    "    \"inputText\": \"Summarize the following:   TOKYO–January 19, 2024–Today, Amazon Web Services (AWS) announced its plans to invest 2.26 trillion yen into its existing cloud infrastructure in Tokyo and Osaka by 2027 to meet growing customer demand for cloud services in Japan. According to the new AWS Economic Impact Study (EIS) for Japan, this planned investment is estimated to contribute 5.57 trillion yen to Japan’s Gross Domestic Product (GDP), and support an estimated average of 30,500 full-time equivalent (FTE) jobs in local Japanese businesses each year. Having already invested 1.51 trillion yen in Japan from 2011 to 2022, AWS’s planned total investment into cloud infrastructure in the country by 2027 will be approximately 3.77 trillion yen. Hundreds of thousands of active customers use the two AWS Regions in Japan to digitally transform (DX) their businesses. AWS opened its first office in Japan in 2009 and launched the AWS Asia Pacific (Tokyo) Region in 2011, and the AWS Asia Pacific (Osaka) Region in 2021. As demand for cloud services to drive the government’s DX agenda grew in Japan, AWS invested 1.51 trillion yen between 2011 and 2022 to construct, connect, operate, and maintain AWS data centers. This is estimated to have contributed 1.46 trillion yen to Japan’s GDP and supported more than 7,100 FTE jobs. These positions, including construction, facility maintenance, engineering, telecommunications, and other jobs within the country’s broader economy, are part of the AWS data center supply chain in Japan.\",\n",
    "    \"textGenerationConfig\": {\n",
    "        \"temperature\": 0.01,  \n",
    "        \"topP\": 0.99,\n",
    "        \"maxTokenCount\": 300\n",
    "    }\n",
    "})\n",
    "\n",
    "# Specify content types for request and response\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n",
    "\n",
    "# Invoke custom model with the provided parameters\n",
    "response = bedrockRuntime.invoke_model(body=body, modelId=provisionedModelArn, accept=accept, contentType=contentType)\n",
    "\n",
    "# Parse and print the output from the custom model\n",
    "response_body_custom = json.loads(response.get('body').read())\n",
    "print(\"Custom Model Output:\")\n",
    "print(response_body_custom['results'][0]['outputText'])\n",
    "\n",
    "# Invoke the base model with the same parameters\n",
    "response = bedrockRuntime.invoke_model(body=body, modelId=basemodelId, accept=accept, contentType=contentType)\n",
    "\n",
    "# Parse and print the output from the base model\n",
    "response_body_base = json.loads(response.get('body').read())\n",
    "print(\"\\n\")\n",
    "print(\"Base Model Output:\")\n",
    "print(response_body_base['results'][0]['outputText'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the provisioned capacity and the custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the provisioned capacity\n",
    "bedrock.delete_provisioned_model_throughput(provisionedModelId=provisionedModelArn)\n",
    "\n",
    "# Delete the custom model\n",
    "bedrock.delete_custom_model (modelIdentifier=customModelId)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
