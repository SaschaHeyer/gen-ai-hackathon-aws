{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8833e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary modules for embeddings, LLMs, and document loading\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "import boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c451fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the PDF documents with PyPDFLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load and split the API and User Guide PDF files\n",
    "loader = PyPDFLoader(\"/Users/sascha/Desktop/development/gen-ai-hackathon-aws/hackathon-workshop/files/bedrock-api.pdf\")\n",
    "api_pages = loader.load_and_split()\n",
    "\n",
    "loader = PyPDFLoader(\"/Users/sascha/Desktop/development/gen-ai-hackathon-aws/hackathon-workshop/files/bedrock-ug.pdf\")\n",
    "ug_pages = loader.load_and_split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1232\n",
      "1232\n"
     ]
    }
   ],
   "source": [
    "print(len(api_pages))\n",
    "print(len(ug_pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b10ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import JSON and initialize the Bedrock client\n",
    "import json\n",
    "bedrockruntime = boto3.client('bedrock-runtime')\n",
    "\n",
    "# Initialize Bedrock embeddings using the Amazon Titan model\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=bedrockruntime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e520022",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a vectorstore with Chroma from the loaded documents\n",
    "from langchain.vectorstores import chroma\n",
    "\n",
    "bigvectorstore_chroma = chroma.Chroma.from_documents(\n",
    "    api_pages + ug_pages,\n",
    "    bedrock_embeddings\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16d4d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import modules for the retrieval-based QA system and prompt templates\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define the prompt template for the QA model\n",
    "prompt_template = \"\"\"\n",
    "Human: Use the following pieces of context to provide an accurate answer to the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be8778dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the LLM using the Bedrock API and Anthropic Claude-v2 model\n",
    "llm = Bedrock(model_id=\"anthropic.claude-v2\", client=bedrockruntime, model_kwargs={'max_tokens_to_sample': 1024})\n",
    "\n",
    "# Create the prompt template object\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcc9c99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the RetrievalQA chain with the vectorstore and the LLM\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=bigvectorstore_chroma.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 5}\n",
    "    ),\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19bf835c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the query to be answered by the RetrievalQA chain\n",
    "query = '''How to invoke a bedrock model with boto3? Provide a concrete example'''\n",
    "\n",
    "# Get the result from the RetrievalQA chain\n",
    "result = qa({\"query\": query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01c9522a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Here is an example of how to invoke a Bedrock model using the Boto3 Python SDK:\n",
       "\n",
       "```python\n",
       "import boto3\n",
       "\n",
       "bedrock = boto3.client('bedrock')\n",
       "\n",
       "response = bedrock.create_model_invocation_job(\n",
       "    JobName='MyInferenceJob',\n",
       "    ModelId='abc123', \n",
       "    InputDataConfig={\n",
       "        'S3Uri': 's3://mybucket/input.jsonl'  \n",
       "    },\n",
       "    OutputDataConfig={\n",
       "        'S3Uri': 's3://mybucket/output/'\n",
       "    },\n",
       "    RoleArn='arn:aws:iam::123456789012:role/BedrockAccessRole'\n",
       ")\n",
       "\n",
       "print(response['JobArn'])\n",
       "```\n",
       "\n",
       "The key steps are:\n",
       "\n",
       "- Create a Bedrock client \n",
       "- Call `create_model_invocation_job()`\n",
       "- Pass the model ID, input/output S3 locations, and an IAM role \n",
       "- The response will contain the `JobArn` to monitor the status of the batch inference job\n",
       "\n",
       "So this shows how to kick off an asynchronous inference job in Bedrock using Boto3. You would then need to check the job status and retrieve the results from S3 once it completes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Display the result in Markdown format\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(result['result']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
